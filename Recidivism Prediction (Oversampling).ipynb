{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UB4lo6lRASNc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns \n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "81WGEGqZAbBk",
    "outputId": "0d3fcee3-e840-4566-e668-5520fbfdf2b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 42)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>group</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>familyincome</th>\n",
       "      <th>offender</th>\n",
       "      <th>SAVRY1</th>\n",
       "      <th>SAVRY2</th>\n",
       "      <th>SAVRY3</th>\n",
       "      <th>...</th>\n",
       "      <th>SAVRY24</th>\n",
       "      <th>I</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P</th>\n",
       "      <th>Reoffending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5010005</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3010071</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010042</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3010047</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010025</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    number  group  age  gender  education  familyincome  offender  SAVRY1  \\\n",
       "0  5010005      1   16       1        4.0             1         4       1   \n",
       "1  3010071      1   18       1        8.0             1         4       2   \n",
       "2  2010042      1   17       1        7.0             1         5       3   \n",
       "3  3010047      1   17       1        5.0             1         4       3   \n",
       "4  2010025      1   17       1        8.0             1         5       3   \n",
       "\n",
       "   SAVRY2  SAVRY3  ...  SAVRY24    I  P1  P2  P3  P4  P5  P6  P  Reoffending  \n",
       "0       2       2  ...        3  1.0   2   2   2   1   2   2  5            0  \n",
       "1       1       1  ...        1  NaN   2   2   2   1   2   1  4            0  \n",
       "2       2       2  ...        3  NaN   2   2   1   1   2   1  3            0  \n",
       "3       2       3  ...        2  0.0   1   2   1   1   1   2  2            0  \n",
       "4       1       1  ...        1  0.0   1   2   1   1   2   2  3            0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('chinareci.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umh_-QvdCFq5"
   },
   "source": [
    "# ***Data Overview***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.drop(['number', 'group','gender','H','S','I','P'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = ProfileReport(df)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZfFFjyOCy7L",
    "outputId": "8fd6a75b-bf5b-4818-8f8e-663b6d86c2ce"
   },
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>familyincome</th>\n",
       "      <th>offender</th>\n",
       "      <th>SAVRY1</th>\n",
       "      <th>SAVRY2</th>\n",
       "      <th>SAVRY3</th>\n",
       "      <th>SAVRY4</th>\n",
       "      <th>SAVRY5</th>\n",
       "      <th>SAVRY6</th>\n",
       "      <th>...</th>\n",
       "      <th>SAVRY22</th>\n",
       "      <th>SAVRY23</th>\n",
       "      <th>SAVRY24</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>Reoffending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>246.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>246.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.764228</td>\n",
       "      <td>7.318367</td>\n",
       "      <td>2.585366</td>\n",
       "      <td>3.540650</td>\n",
       "      <td>2.560976</td>\n",
       "      <td>1.922764</td>\n",
       "      <td>1.995935</td>\n",
       "      <td>1.682927</td>\n",
       "      <td>1.138211</td>\n",
       "      <td>1.686992</td>\n",
       "      <td>...</td>\n",
       "      <td>1.577236</td>\n",
       "      <td>1.930894</td>\n",
       "      <td>2.329268</td>\n",
       "      <td>1.345528</td>\n",
       "      <td>1.577236</td>\n",
       "      <td>1.430894</td>\n",
       "      <td>1.491870</td>\n",
       "      <td>1.731707</td>\n",
       "      <td>1.369919</td>\n",
       "      <td>0.256098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.982137</td>\n",
       "      <td>1.841203</td>\n",
       "      <td>0.951277</td>\n",
       "      <td>1.104894</td>\n",
       "      <td>0.665737</td>\n",
       "      <td>0.846439</td>\n",
       "      <td>0.785054</td>\n",
       "      <td>0.780550</td>\n",
       "      <td>0.400514</td>\n",
       "      <td>0.666696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664040</td>\n",
       "      <td>0.744573</td>\n",
       "      <td>0.783319</td>\n",
       "      <td>0.476510</td>\n",
       "      <td>0.495006</td>\n",
       "      <td>0.496211</td>\n",
       "      <td>0.500953</td>\n",
       "      <td>0.443974</td>\n",
       "      <td>0.483767</td>\n",
       "      <td>0.437366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              age   education  familyincome    offender      SAVRY1  \\\n",
       "count  246.000000  245.000000    246.000000  246.000000  246.000000   \n",
       "mean    16.764228    7.318367      2.585366    3.540650    2.560976   \n",
       "std      0.982137    1.841203      0.951277    1.104894    0.665737   \n",
       "min     14.000000    1.000000      0.000000    1.000000    1.000000   \n",
       "25%     16.000000    7.000000      2.000000    3.000000    2.000000   \n",
       "50%     17.000000    8.000000      2.500000    4.000000    3.000000   \n",
       "75%     17.000000    9.000000      3.000000    4.000000    3.000000   \n",
       "max     18.000000   11.000000      5.000000    5.000000    3.000000   \n",
       "\n",
       "           SAVRY2      SAVRY3      SAVRY4      SAVRY5      SAVRY6  ...  \\\n",
       "count  246.000000  246.000000  246.000000  246.000000  246.000000  ...   \n",
       "mean     1.922764    1.995935    1.682927    1.138211    1.686992  ...   \n",
       "std      0.846439    0.785054    0.780550    0.400514    0.666696  ...   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000  ...   \n",
       "25%      1.000000    1.000000    1.000000    1.000000    1.000000  ...   \n",
       "50%      2.000000    2.000000    1.000000    1.000000    2.000000  ...   \n",
       "75%      3.000000    3.000000    2.000000    1.000000    2.000000  ...   \n",
       "max      3.000000    3.000000    3.000000    3.000000    3.000000  ...   \n",
       "\n",
       "          SAVRY22     SAVRY23     SAVRY24          P1          P2          P3  \\\n",
       "count  246.000000  246.000000  246.000000  246.000000  246.000000  246.000000   \n",
       "mean     1.577236    1.930894    2.329268    1.345528    1.577236    1.430894   \n",
       "std      0.664040    0.744573    0.783319    0.476510    0.495006    0.496211   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "25%      1.000000    1.000000    2.000000    1.000000    1.000000    1.000000   \n",
       "50%      1.000000    2.000000    3.000000    1.000000    2.000000    1.000000   \n",
       "75%      2.000000    2.000000    3.000000    2.000000    2.000000    2.000000   \n",
       "max      3.000000    3.000000    3.000000    2.000000    2.000000    2.000000   \n",
       "\n",
       "               P4          P5          P6  Reoffending  \n",
       "count  246.000000  246.000000  246.000000   246.000000  \n",
       "mean     1.491870    1.731707    1.369919     0.256098  \n",
       "std      0.500953    0.443974    0.483767     0.437366  \n",
       "min      1.000000    1.000000    1.000000     0.000000  \n",
       "25%      1.000000    1.000000    1.000000     0.000000  \n",
       "50%      1.000000    2.000000    1.000000     0.000000  \n",
       "75%      2.000000    2.000000    2.000000     1.000000  \n",
       "max      2.000000    2.000000    2.000000     1.000000  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary Statistics for Numerical data:\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age             0\n",
       "education       1\n",
       "familyincome    0\n",
       "offender        0\n",
       "SAVRY1          0\n",
       "SAVRY2          0\n",
       "SAVRY3          0\n",
       "SAVRY4          0\n",
       "SAVRY5          0\n",
       "SAVRY6          0\n",
       "SAVRY7          0\n",
       "SAVRY8          0\n",
       "SAVRY9          0\n",
       "SAVRY10         0\n",
       "SAVRY11         0\n",
       "SAVRY12         0\n",
       "SAVRY13         0\n",
       "SAVRY14         0\n",
       "SAVRY15         0\n",
       "SAVRY16         0\n",
       "SAVRY17         0\n",
       "SAVRY18         0\n",
       "SAVRY19         0\n",
       "SAVRY20         0\n",
       "SAVRY21         0\n",
       "SAVRY22         0\n",
       "SAVRY23         0\n",
       "SAVRY24         0\n",
       "P1              0\n",
       "P2              0\n",
       "P3              0\n",
       "P4              0\n",
       "P5              0\n",
       "P6              0\n",
       "Reoffending     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use isnull().sum() to check for missing values \n",
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"education\"] = df[\"education\"].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age             0\n",
       "education       0\n",
       "familyincome    0\n",
       "offender        0\n",
       "SAVRY1          0\n",
       "SAVRY2          0\n",
       "SAVRY3          0\n",
       "SAVRY4          0\n",
       "SAVRY5          0\n",
       "SAVRY6          0\n",
       "SAVRY7          0\n",
       "SAVRY8          0\n",
       "SAVRY9          0\n",
       "SAVRY10         0\n",
       "SAVRY11         0\n",
       "SAVRY12         0\n",
       "SAVRY13         0\n",
       "SAVRY14         0\n",
       "SAVRY15         0\n",
       "SAVRY16         0\n",
       "SAVRY17         0\n",
       "SAVRY18         0\n",
       "SAVRY19         0\n",
       "SAVRY20         0\n",
       "SAVRY21         0\n",
       "SAVRY22         0\n",
       "SAVRY23         0\n",
       "SAVRY24         0\n",
       "P1              0\n",
       "P2              0\n",
       "P3              0\n",
       "P4              0\n",
       "P5              0\n",
       "P6              0\n",
       "Reoffending     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    183\n",
       "1     63\n",
       "Name: Reoffending, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Reoffending\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>familyincome</th>\n",
       "      <th>offender</th>\n",
       "      <th>SAVRY1</th>\n",
       "      <th>SAVRY2</th>\n",
       "      <th>SAVRY3</th>\n",
       "      <th>SAVRY4</th>\n",
       "      <th>SAVRY5</th>\n",
       "      <th>SAVRY6</th>\n",
       "      <th>...</th>\n",
       "      <th>SAVRY22</th>\n",
       "      <th>SAVRY23</th>\n",
       "      <th>SAVRY24</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>Reoffending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  education  familyincome  offender  SAVRY1  SAVRY2  SAVRY3  SAVRY4  \\\n",
       "0   16        4.0             1         4       1       2       2       1   \n",
       "1   18        8.0             1         4       2       1       1       1   \n",
       "2   17        7.0             1         5       3       2       2       2   \n",
       "3   17        5.0             1         4       3       2       3       2   \n",
       "4   17        8.0             1         5       3       1       1       2   \n",
       "\n",
       "   SAVRY5  SAVRY6  ...  SAVRY22  SAVRY23  SAVRY24  P1  P2  P3  P4  P5  P6  \\\n",
       "0       1       1  ...        2        1        3   2   2   2   1   2   2   \n",
       "1       1       1  ...        2        1        1   2   2   2   1   2   1   \n",
       "2       1       1  ...        1        3        3   2   2   1   1   2   1   \n",
       "3       2       1  ...        2        2        2   1   2   1   1   1   2   \n",
       "4       1       2  ...        2        1        1   1   2   1   1   2   2   \n",
       "\n",
       "   Reoffending  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplots(figsize =(8,5))\n",
    "df['age'].value_counts(normalize=True)\n",
    "df['age'].value_counts(dropna = False).plot.bar(color =['green','orange','purple'])\n",
    "plt.title('Comparison of various groups')\n",
    "plt.xlabel('AGE')\n",
    "plt.ylabel('number of respondents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns \n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_14= df[df['age']==14]\n",
    "age_15= df[df['age']==15]\n",
    "age_16= df[df['age']==16]\n",
    "age_17= df[df['age']==17]\n",
    "age_18 = df[df['age']==18]\n",
    "\n",
    "labels = ['14','15','16','17','18']\n",
    "values = [len(age_14), len(age_15),len(age_16),len(age_17),len(age_18)]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\n",
    "fig.update_layout(\n",
    "    title_text=\"Analysis on - Age\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Reoffending\",data=df,hue=\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low= df[df['SAVRY17']==1]\n",
    "moderate = df[df['SAVRY17']==2]\n",
    "high = df[df['SAVRY17']==3]\n",
    "\n",
    "labels = ['Low','moderate','high']\n",
    "values = [len(low), len(moderate),len(high)]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\n",
    "fig.update_layout(\n",
    "    title_text=\"Negative attitudes\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Reoffending\",data=df,hue=\"SAVRY17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low= df[df['SAVRY4']==1]\n",
    "moderate = df[df['SAVRY4']==2]\n",
    "high = df[df['SAVRY4']==3]\n",
    "\n",
    "labels = ['Low','moderate','high']\n",
    "values = [len(low), len(moderate),len(high)]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\n",
    "fig.update_layout(\n",
    "    title_text=\"Past supervision failures\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Reoffending\",data=df,hue=\"SAVRY4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_poor= df[df['familyincome']==1]\n",
    "poor = df[df['familyincome']==2]\n",
    "middle_class = df[df['familyincome']==3]\n",
    "rich = df[df['familyincome']==4]\n",
    "very_rich = df[df['familyincome']==5]\n",
    "no_income = df[df['familyincome']==0]\n",
    "\n",
    "\n",
    "labels = ['very poor','poor','middle class','rich','very rich','no income']\n",
    "values = [len(very_poor), len(poor),len(middle_class),len(rich),len(very_rich),len(no_income)]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\n",
    "fig.update_layout(\n",
    "    title_text=\"Family income\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Reoffending\",data=df,hue=\"familyincome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create filtered data frames\n",
    "filtered_dfs = []\n",
    "filtered_dfs.append(df[df['education'] == 1.0])\n",
    "filtered_dfs.append(df[df['education'] == 2.0])\n",
    "filtered_dfs.append(df[df['education'] == 3.0])\n",
    "filtered_dfs.append(df[df['education'] == 4.0])\n",
    "filtered_dfs.append(df[df['education'] == 5.0])\n",
    "filtered_dfs.append(df[df['education'] == 6.0])\n",
    "filtered_dfs.append(df[df['education'] == 7.0])\n",
    "filtered_dfs.append(df[df['education'] == 8.0])\n",
    "filtered_dfs.append(df[df['education'] == 9.0])\n",
    "filtered_dfs.append(df[df['education'] == 10.0])\n",
    "filtered_dfs.append(df[df['education'] == 11.0])\n",
    "\n",
    "# Compute the lengths of each filtered data frame\n",
    "values = [len(filtered_df) for filtered_df in filtered_dfs]\n",
    "\n",
    "# Define labels\n",
    "labels = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
    "\n",
    "# Create the pie chart\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=0.4)])\n",
    "fig.update_layout(title_text=\"Educational Background\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Reoffending\",data=df,hue=\"education\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low= df[df['SAVRY20']==1]\n",
    "moderate = df[df['SAVRY20']==2]\n",
    "high = df[df['SAVRY20']==3]\n",
    "\n",
    "labels = ['Low','moderate','high']\n",
    "values = [len(low), len(moderate),len(high)]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\n",
    "fig.update_layout(\n",
    "    title_text=\"Anger management problems\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Reoffending\",data=df,hue=\"SAVRY20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low= df[df['SAVRY24']==1]\n",
    "moderate = df[df['SAVRY24']==2]\n",
    "high = df[df['SAVRY24']==3]\n",
    "\n",
    "labels = ['Low','moderate','high']\n",
    "values = [len(low), len(moderate),len(high)]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\n",
    "fig.update_layout(\n",
    "    title_text=\"Low commitment to school or work\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Reoffending\",data=df,hue=\"SAVRY24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low= df[df['SAVRY2']==1]\n",
    "moderate = df[df['SAVRY2']==2]\n",
    "high = df[df['SAVRY2']==3]\n",
    "\n",
    "labels = ['Low','moderate','high']\n",
    "values = [len(low), len(moderate),len(high)]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\n",
    "fig.update_layout(\n",
    "    title_text=\"Analysis on non-violent offending\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a countplot with customized hue label\n",
    "sns.countplot(x=\"Reoffending\", data=df, hue=\"SAVRY2\")\n",
    "\n",
    "# Set the customized hue label\n",
    "plt.legend(title=\"History of non violent offending\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low= df[df['SAVRY16']==1]\n",
    "moderate =df[df['SAVRY16']==2]\n",
    "high = df[df['SAVRY16']==3]\n",
    "\n",
    "labels = ['Low','moderate','high']\n",
    "values = [len(low), len(moderate),len(high)]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\n",
    "fig.update_layout(\n",
    "    title_text=\"Community of disorganization\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot a countplot with customized hue label\n",
    "sns.countplot(x=\"Reoffending\", data=df, hue=\"SAVRY16\")\n",
    "\n",
    "# Set the customized hue label\n",
    "plt.legend(title=\"Community of disorganization\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot a countplot with customized hue label\n",
    "sns.countplot(x=\"Reoffending\", data=df, hue=\"SAVRY24\")\n",
    "\n",
    "# Set the customized hue label\n",
    "plt.legend(title=\"low interest to school\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F_6gmh_tC60p"
   },
   "outputs": [],
   "source": [
    "# Separate the independent (X) and dependent (y) features\n",
    "X = df.drop(columns = 'Reoffending')\n",
    "y = df['Reoffending']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Reoffending'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show pie plot (Approach 1)\n",
    "y.value_counts().plot.pie(autopct='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (F:\\ANACON\\lib\\site-packages\\sklearn\\utils\\_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#ros = RandomOverSampler(sampling_strategy=1) # Float\u001b[39;00m\n\u001b[0;32m      4\u001b[0m ros \u001b[38;5;241m=\u001b[39m RandomOverSampler(sampling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot majority\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# String\u001b[39;00m\n",
      "File \u001b[1;32mF:\\ANACON\\lib\\site-packages\\imblearn\\__init__.py:52\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32mF:\\ANACON\\lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mF:\\ANACON\\lib\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[1;32mF:\\ANACON\\lib\\site-packages\\imblearn\\base.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[1;32mF:\\ANACON\\lib\\site-packages\\imblearn\\utils\\_param_validation.py:908\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    909\u001b[0m     HasMethods,\n\u001b[0;32m    910\u001b[0m     Hidden,\n\u001b[0;32m    911\u001b[0m     Interval,\n\u001b[0;32m    912\u001b[0m     Options,\n\u001b[0;32m    913\u001b[0m     StrOptions,\n\u001b[0;32m    914\u001b[0m     _ArrayLikes,\n\u001b[0;32m    915\u001b[0m     _Booleans,\n\u001b[0;32m    916\u001b[0m     _Callables,\n\u001b[0;32m    917\u001b[0m     _CVObjects,\n\u001b[0;32m    918\u001b[0m     _InstancesOf,\n\u001b[0;32m    919\u001b[0m     _IterablesNotString,\n\u001b[0;32m    920\u001b[0m     _MissingValues,\n\u001b[0;32m    921\u001b[0m     _NoneConstraint,\n\u001b[0;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[0;32m    923\u001b[0m     _RandomStates,\n\u001b[0;32m    924\u001b[0m     _SparseMatrices,\n\u001b[0;32m    925\u001b[0m     _VerboseHelper,\n\u001b[0;32m    926\u001b[0m     make_constraint,\n\u001b[0;32m    927\u001b[0m     validate_params,\n\u001b[0;32m    928\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (F:\\ANACON\\lib\\site-packages\\sklearn\\utils\\_param_validation.py)"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#ros = RandomOverSampler(sampling_strategy=1) # Float\n",
    "ros = RandomOverSampler(sampling_strategy=\"not majority\") # String\n",
    "X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "ax = y_res.value_counts().plot.pie(autopct='%.2f')\n",
    "_ = ax.set_title(\"Over-sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - Test Split (Over Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRq5NsecDBGb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_oversampling, X_test_oversampling, y_train_oversampling, y_test_oversampling = train_test_split(X_res, y_res, test_size = 0.2, random_state = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization (Min-Max Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_oversampling)\n",
    "X_train_scaled_oversampling = scaler.transform(X_train_oversampling)\n",
    "X_test_scaled_oversampling = scaler.transform(X_test_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_oversampling = pd.DataFrame(X_train_scaled_oversampling, columns=X_train_oversampling.columns)\n",
    "X_test_scaled_oversampling = pd.DataFrame(X_test_scaled_oversampling, columns=X_test_oversampling.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(X_train_oversampling.describe(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(X_train_scaled_oversampling.describe(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(X_test_scaled_oversampling.describe(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_oversampling.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values with a small non-zero value\n",
    "small_nonzero_value = 1e-10\n",
    "data_filled = X_test_scaled_oversampling.fillna(small_nonzero_value)\n",
    "data_filled = X_train_scaled_oversampling.fillna(small_nonzero_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_oversampling.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_oversampling.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_oversampling.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_oversampling.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train_oversampling['offender'], ax=ax1)\n",
    "sns.kdeplot(X_train_oversampling['age'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Standard Scaling')\n",
    "sns.kdeplot(X_train_scaled_oversampling['offender'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled_oversampling['age'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count the number of instances in each class after oversampling\n",
    "value_counts_oversampled = pd.Series(y_res).value_counts()\n",
    "\n",
    "# Print the number of instances in each class after oversampling\n",
    "print(\"Class Distribution After Oversampling:\")\n",
    "print(value_counts_oversampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZeroR Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of DummyClassifier with strategy 'most_frequent'\n",
    "z_classifier = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# Training the classifier on the training set\n",
    "z_classifier.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Making predictions on the testing set\n",
    "y_pred = z_classifier.predict(X_test_oversampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have true labels (y_true) and predicted labels (y_pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for Zero R')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Paj_JJTnDfdC"
   },
   "source": [
    "# ***Random Forest Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate the classifier \n",
    "rfc = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# fit the model\n",
    "rfc.fit(X_train_scaled_oversampling,y_train_oversampling)\n",
    "\n",
    "# Predict the Test set results\n",
    "y_pred = rfc.predict(X_test_scaled_oversampling)\n",
    "\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for Random forest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis : Investigate the level of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train_scaled_oversampling,y_train_oversampling)\n",
    "\n",
    "# Make predictions on both the training and test sets\n",
    "y_train_pred = rfc.predict(X_train_scaled_oversampling)\n",
    "y_test_pred = rfc.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Compute the accuracy scores for training and test sets\n",
    "train_accuracy = accuracy_score(y_train_oversampling, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test_oversampling, y_test_pred)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier with n_estimators 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the classifier with n_estimators = 100\n",
    "rfc_100 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# fit the model to the training set\n",
    "rfc_100.fit(X_train_scaled_oversampling,y_train_oversampling)\n",
    "\n",
    "# Predict on the test set results\n",
    "y_pred_100 = rfc_100.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Check accuracy score \n",
    "print('Model accuracy score with 100 decision-trees : {0:0.4f}'. format(accuracy_score(y_test_oversampling, y_pred_100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(rfc_100.score(X_train_scaled_oversampling, y_train_oversampling)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(rfc_100.score(X_test_scaled_oversampling, y_test_oversampling)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration(A lower calibrated Brier score compared to the original Brier score indicates improved calibration performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.calibration import CalibrationDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability diagram without calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predicted probabilities from the model\n",
    "y_pred_probs = rfc.predict_proba(X_test_scaled_oversampling)[:, 1]\n",
    "\n",
    "# Calculate Brier score loss to evaluate calibration quality\n",
    "original_brier_score = brier_score_loss(y_test_oversampling, y_pred_probs)\n",
    "\n",
    "print(\"Original Brier Score:\", original_brier_score)\n",
    "\n",
    "# Compute the fraction of positives and the mean predicted value\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test_oversampling, y_pred_probs, n_bins=10)\n",
    "\n",
    "# Plot reliability diagram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, marker='o', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.7)\n",
    "plt.xlabel('Mean Predicted Value')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Reliability Diagram (No Calibration)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability diagram after sigmoid calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predicted probabilities from the model\n",
    "y_pred_probs = rfc.predict_proba(X_test_scaled_oversampling)[:, 1]\n",
    "\n",
    "# Perform isotonic calibration\n",
    "calibrated_model = CalibratedClassifierCV(rfc, method='sigmoid', cv='prefit')\n",
    "calibrated_model.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Obtain the calibrated predicted probabilities\n",
    "calibrated_pred_probs = calibrated_model.predict_proba(X_test_scaled_oversampling)[:, 1]\n",
    "\n",
    "# Calculate Brier score loss to evaluate calibration quality\n",
    "original_brier_score = brier_score_loss(y_test_oversampling, y_pred_probs)\n",
    "calibrated_brier_score = brier_score_loss(y_test_oversampling, calibrated_pred_probs)\n",
    "\n",
    "print(\"Original Brier Score:\", original_brier_score)\n",
    "print(\"Calibrated Brier Score:\", calibrated_brier_score)\n",
    "\n",
    "# Compute the fraction of positives and the mean predicted value\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test_oversampling, calibrated_pred_probs, n_bins=10)\n",
    "\n",
    "# Plot reliability diagram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, marker='o', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.7)\n",
    "plt.xlabel('Mean Predicted Value')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Reliability Diagram (Sigmoid Calibration)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability diagram after isotonic calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predicted probabilities from the model\n",
    "y_pred_probs = rfc.predict_proba(X_test_scaled_oversampling)[:, 1]\n",
    "\n",
    "# Perform isotonic calibration\n",
    "calibrated_model = CalibratedClassifierCV(rfc, method='isotonic', cv='prefit')\n",
    "calibrated_model.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Obtain the calibrated predicted probabilities\n",
    "calibrated_pred_probs = calibrated_model.predict_proba(X_test_scaled_oversampling)[:, 1]\n",
    "\n",
    "# Calculate Brier score loss to evaluate calibration quality\n",
    "original_brier_score = brier_score_loss(y_test_oversampling, y_pred_probs)\n",
    "calibrated_brier_score = brier_score_loss(y_test_oversampling, calibrated_pred_probs)\n",
    "\n",
    "print(\"Original Brier Score:\", original_brier_score)\n",
    "print(\"Calibrated Brier Score:\", calibrated_brier_score)\n",
    "\n",
    "# Compute the fraction of positives and the mean predicted value\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test_oversampling, calibrated_pred_probs, n_bins=10)\n",
    "\n",
    "# Plot reliability diagram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, marker='o', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.7)\n",
    "plt.xlabel('Mean Predicted Value')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Reliability Diagram (Isotonic Calibration)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the feature scores\n",
    "\n",
    "feature_scores = pd.Series(rfc_100.feature_importances_, index=X_train_scaled_oversampling.columns).sort_values(ascending=False)\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a seaborn bar plot\n",
    "\n",
    "sns.barplot(x=feature_scores, y=feature_scores.index)\n",
    "\n",
    "# Add labels to the graph\n",
    "\n",
    "plt.xlabel('Feature Importance Score')\n",
    "\n",
    "plt.ylabel('Features')\n",
    "\n",
    "# Add title to the graph\n",
    "\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "\n",
    "# Visualize the graph\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier using randomized search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "\n",
    "# Define the parameter distributions for the randomized search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Create a Random Forest Classifier object\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Create a RandomizedSearchCV object with the specified number of iterations\n",
    "n_iter_search = 20\n",
    "rfc_random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search)\n",
    "\n",
    "# Fit the RandomizedSearchCV object to your data\n",
    "start = time()\n",
    "rfc_random_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates parameter settings.\" % ((time() - start), n_iter_search))\n",
    "\n",
    "# Get the best estimator from the RandomizedSearchCV\n",
    "best_estimator = rfc_random_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = best_estimator.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for Random forest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "# Initialize the classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': rfc\n",
    "  \n",
    "}\n",
    "\n",
    "# Define the training set sizes\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)  # Adjust the range according to your needs\n",
    "\n",
    "# Plot the learning curves for each classifier\n",
    "plt.figure(figsize=(10, 8))\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Calculate the learning curve scores\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, X_train_scaled_oversampling, y_train_oversampling, train_sizes=train_sizes, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Calculate the mean of the training and test scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "   \n",
    "\n",
    "    # Plot the learning curve for the current classifier\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=f'{clf_name} (Training Score)')\n",
    "   \n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curves for Random Forest(Randomized Search CV) Algorithms')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': best_estimator\n",
    "   \n",
    "}\n",
    "\n",
    "# Define the training set sizes\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)  # Adjust the range according to your needs\n",
    "\n",
    "# Plot the learning curves for each classifier\n",
    "plt.figure(figsize=(10, 8))\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Calculate the learning curve scores\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, X_train_scaled_oversampling, y_train_oversampling, train_sizes=train_sizes, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Calculate the mean of the training and test scores\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "   \n",
    "\n",
    "    # Plot the learning curve for the current classifier\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', label=f'{clf_name} (Testing Score)')\n",
    "   \n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Test Set Size')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Validation Curves for Random Forest(Randomized Search CV) Algorithms')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP on Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 44\n",
    "end_index = 68\n",
    "shap_values = explainer.shap_values(X_test_scaled_oversampling[start_index:end_index])\n",
    "X_test_scaled_oversampling[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_values[0].shape)\n",
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% >> Visualize local predictions\n",
    "shap.initjs()\n",
    "# Force plot\n",
    "prediction = rfc.predict(X_test_scaled_oversampling[start_index:end_index])[0]\n",
    "print(f\"The RF predicted: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1],\n",
    "                shap_values[1],\n",
    "                X_test_scaled_oversampling[start_index:end_index]) # for values\n",
    "\n",
    "# %% >> Visualize global features\n",
    "# Feature summary\n",
    "shap.summary_plot(shap_values, X_test_scaled_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier using grid search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X_train_scaled_oversampling, y_train_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfc.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc1=RandomForestClassifier(random_state=42, max_features='log2', n_estimators= 200, max_depth=5, criterion='gini')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc1.fit(X_train_scaled_oversampling, y_train_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=rfc1.predict(X_test_scaled_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for Random Forest on CV data: \",accuracy_score(y_test_oversampling,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier using hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier using bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Assuming you have feature data 'X' and target data 'y'\n",
    "\n",
    "# Define the evaluation function for Bayesian Optimization\n",
    "def evaluate_model(n_estimators, max_depth, min_samples_split):\n",
    "    # Create a Random Forest classifier with the specified hyperparameters\n",
    "    model = rfc\n",
    "\n",
    "    # Perform cross-validation with 5 folds\n",
    "    scores = cross_val_score(model, X_train_scaled_oversampling, y_train_oversampling, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Return the average accuracy score\n",
    "    return scores.mean()\n",
    "\n",
    "# Define the parameter ranges for Bayesian Optimization\n",
    "param_ranges = {\n",
    "    'n_estimators': (10, 1000),\n",
    "    'max_depth': (1, 50),\n",
    "    'min_samples_split': (2, 10)\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "optimizer = BayesianOptimization(evaluate_model, param_ranges)\n",
    "optimizer.maximize(n_iter=10, init_points=5)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = optimizer.max['params']\n",
    "best_n_estimators = best_params['n_estimators']\n",
    "best_max_depth = best_params['max_depth']\n",
    "best_min_samples_split = best_params['min_samples_split']\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = RandomForestClassifier(\n",
    "    n_estimators=int(best_n_estimators),\n",
    "    max_depth=int(best_max_depth),\n",
    "    min_samples_split=int(best_min_samples_split)\n",
    ")\n",
    "final_model.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = final_model.predict(X_train_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_train_oversampling, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(X_train_scaled_oversampling, y_train_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have already fitted your TPOT classifier and have test data (X_test, y_test)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = tpot.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier using Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=100)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "    # Initialize and train the Random Forest Classifier\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42,\n",
    "    )\n",
    "    clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "    # Evaluate the classifier on the validation set\n",
    "    y_pred = clf.predict(X_test_oversampling)\n",
    "\n",
    "    # Return the accuracy score as the objective value for Optuna to optimize\n",
    "    return 1.0 - accuracy_score(y_test_oversampling, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters found by Optuna\n",
    "best_params = study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_clf.predict(X_test_scaled_oversampling)\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a logistic regression model on the training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "\n",
    "\n",
    "# fit the model\n",
    "logreg.fit(X_train_scaled_oversampling,y_train_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = logreg.predict(X_test_scaled_oversampling)\n",
    "\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of getting output as 0 - not reoffending\n",
    "\n",
    "logreg.predict_proba(X_test_scaled_oversampling)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of getting output as 1 - reoffending\n",
    "\n",
    "logreg.predict_proba(X_test_scaled_oversampling)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test_oversampling, y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = logreg.predict(X_train_scaled_oversampling)\n",
    "\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train_oversampling, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set(Check for overfitting and underfitting)\n",
    "\n",
    "print('Training set score: {:.4f}'.format(logreg.score(X_train_scaled_oversampling, y_train_oversampling)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(logreg.score(X_test_scaled_oversampling, y_test_oversampling)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =logreg.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for logistic regression')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the Logsitic Regression model with C=100\n",
    "\n",
    "# instantiate the model\n",
    "logreg100 = LogisticRegression(C=100, solver='liblinear', random_state=0)\n",
    "\n",
    "\n",
    "# fit the model\n",
    "logreg100.fit(X_train_scaled_oversampling, y_train_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(logreg100.score(X_train_scaled_oversampling, y_train_oversampling)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(logreg100.score(X_test_scaled_oversampling, y_test_oversampling)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression using GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "parameters = [{'penalty':['l1','l2']}, \n",
    "              {'C':[1, 10, 100, 1000]}]\n",
    "\n",
    "\n",
    "\n",
    "log_grid_search = GridSearchCV(estimator = logreg,  \n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           verbose=0)\n",
    "\n",
    "\n",
    "log_grid_search.fit(X_train_scaled_oversampling, y_train_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the best model\n",
    "\n",
    "# best score achieved during the GridSearchCV\n",
    "print('GridSearch CV best score : {:.4f}\\n\\n'.format(log_grid_search.best_score_))\n",
    "\n",
    "# print parameters that give the best results\n",
    "print('Parameters that give the best results :','\\n\\n', (log_grid_search.best_params_))\n",
    "\n",
    "# print estimator that was chosen by the GridSearch\n",
    "print('\\n\\nEstimator that was chosen by the search :','\\n\\n', (log_grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate GridSearch CV score on test set\n",
    "\n",
    "print('GridSearch CV score on test set: {0:0.4f}'.format(log_grid_search.score(X_test_scaled_oversampling, y_test_oversampling)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_grid_search.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression using randomized search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = range(100, 500)\n",
    "solver = ['lbfgs', 'newton-cg', 'liblinear']\n",
    "warm_start = [True, False]\n",
    "C = np.arange(0, 1, 0.01)\n",
    "random_grid ={\n",
    "    'max_iter' : max_iter,\n",
    "    'warm_start' : warm_start,\n",
    "    'solver' : solver,\n",
    "    'C' : C,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_estimator = RandomizedSearchCV(estimator = logreg,\n",
    "                                   param_distributions = random_grid,\n",
    "                                   n_iter = 100,\n",
    "                                   scoring = 'accuracy',\n",
    "                                   n_jobs = -1,\n",
    "                                   verbose = 1, \n",
    "                                   random_state = 1,\n",
    "                                  )\n",
    "\n",
    "random_estimator.fit(X_test_scaled_oversampling, y_test_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_estimator.best_params_\n",
    "\n",
    "best_estimator = random_estimator.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_test_scaled_oversampling, y_test_oversampling)\n",
    "\n",
    "pred = best_estimator.predict(X_test_scaled_oversampling)\n",
    "\n",
    "accuracy_score(pred, y_test_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test_oversampling,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "predicted = best_estimator.predict(X_test_scaled_oversampling)\n",
    "print(\"Test score: {:.2f}\".format(accuracy_score(y_test_oversampling,predicted)))\n",
    "print(\"Cohen Kappa score: {:.2f}\".format(cohen_kappa_score(y_test_oversampling,predicted)))\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = sns.heatmap(confusion_matrix(y_test_oversampling,predicted),annot=True)\n",
    "ax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n",
    "            xticklabels=(['True', 'False']),\n",
    "            yticklabels=(['True', 'False']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for Zero R')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression using optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    C = trial.suggest_loguniform(\"C\", 1e-5, 1e5)\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"liblinear\", \"saga\"])\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "\n",
    "    # Initialize and train the Logistic Regression model\n",
    "    clf = LogisticRegression(\n",
    "        C=C, penalty=penalty, solver=solver, max_iter=max_iter, random_state=42\n",
    "    )\n",
    "    clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "    # Evaluate the classifier on the validation set\n",
    "    y_pred = clf.predict(X_test_scaled_oversampling)\n",
    "\n",
    "    # Return the accuracy score as the objective value for Optuna to optimize\n",
    "    return 1.0 - (y_pred == y_test_oversampling).mean()\n",
    "\n",
    "# Run the optimization process\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters found by Optuna\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the Logistic Regression model with the best hyperparameters\n",
    "best_clf = LogisticRegression(**best_params, random_state=42)\n",
    "best_clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "y_pred = best_clf.predict(X_test_scaled_oversampling)\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Create and fit the TPOT classifier\n",
    "tpot = TPOTClassifier(generations=10, population_size=50, verbosity=2, random_state=42)\n",
    "tpot.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Evaluate the best pipeline on the test set\n",
    "y_pred = tpot.predict(X_test_scaled_oversampling)\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for logistic regression')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression using bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "# Define the objective function for Bayesian optimization\n",
    "def objective(C, penalty, solver, max_iter):\n",
    "    # Convert hyperparameters to their appropriate types\n",
    "    C = 10 ** C\n",
    "    penalty = 'l1' if penalty < 0.5 else 'l2'\n",
    "    solver = 'liblinear' if solver < 0.5 else 'saga'\n",
    "    max_iter = int(max_iter)\n",
    "\n",
    "    # Create and train the Logistic Regression model with the given hyperparameters\n",
    "    clf = LogisticRegression(C=C, penalty=penalty, solver=solver, max_iter=max_iter)\n",
    "    clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "    # Evaluate the classifier on the validation set and return the negative accuracy\n",
    "    y_pred = clf.predict(X_test_scaled_oversampling)\n",
    "    accuracy = (y_pred == y_test_oversampling).mean()\n",
    "    return -accuracy\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "pbounds = {\n",
    "    'C': (-5, 2),\n",
    "    'penalty': (0, 1),\n",
    "    'solver': (0, 1),\n",
    "    'max_iter': (100, 500)\n",
    "}\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "optimizer = BayesianOptimization(f=objective, pbounds=pbounds, random_state=42)\n",
    "optimizer.maximize(init_points=10, n_iter=50)\n",
    "\n",
    "# Get the best hyperparameters found by Bayesian optimization\n",
    "best_params = optimizer.max['params']\n",
    "\n",
    "# Train the Logistic Regression model with the best hyperparameters\n",
    "best_C = 10 ** best_params['C']\n",
    "best_penalty = 'l1' if best_params['penalty'] < 0.5 else 'l2'\n",
    "best_solver = 'liblinear' if best_params['solver'] < 0.5 else 'saga'\n",
    "best_max_iter = int(best_params['max_iter'])\n",
    "best_clf = LogisticRegression(C=best_C, penalty=best_penalty, solver=best_solver, max_iter=best_max_iter)\n",
    "best_clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "y_pred = best_clf.predict(X_test_scaled_oversampling)\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KNeighbors ClaSSifier from sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "\n",
    "# fit the model to the training set\n",
    "knn.fit(X_train_scaled_oversampling, y_train_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test_scaled_oversampling)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test_oversampling, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = knn.predict(X_train_scaled_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train_oversampling, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(knn.score(X_train_scaled_oversampling, y_train_oversampling)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(knn.score(X_test_scaled_oversampling, y_test_oversampling)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for KNN')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN using randomized search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding a best value of k for better accuracy\n",
    "from sklearn import metrics\n",
    "k_range=range(1,26)\n",
    "scores=[]\n",
    "\n",
    "for k in k_range:\n",
    "    knn=KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled_oversampling,y_train_oversampling)\n",
    "    y_pred=knn.predict(X_test_scaled_oversampling)\n",
    "    scores.append(metrics.accuracy_score(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the parameter values that should be searched\n",
    "k_range=list(range(1,31))\n",
    "options=['uniform', 'distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = dict(n_neighbors=k_range, weights=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_iter: Number of random combinations it would try\n",
    "#random_state: Reproducibility/ Set seed\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "rand = RandomizedSearchCV(knn, param_dist, cv=10, scoring='accuracy', n_iter=10, random_state=5)\n",
    "rand.fit(X_train_scaled_oversampling,y_train_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand.best_score_)\n",
    "print(rand.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run RandomizedSearchCV 20 times (n_iter=10) and record the best score\n",
    "\n",
    "best_scores=[]\n",
    "for i in range(20):\n",
    "    rand = RandomizedSearchCV(knn, param_dist, cv=10, scoring='accuracy', n_iter=10)\n",
    "    rand.fit(X_train_scaled_oversampling,y_train_oversampling)\n",
    "    best_scores.append(round(rand.best_score_,3))\n",
    "print(best_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rand.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [3, 4, 5, 6, 7, 8, 9, 10],  # Example values for n_neighbors\n",
    "    'weights': ['uniform', 'distance'],  # Example values for weights\n",
    "    'p': [1, 2]  # Example values for p\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid_search = GridSearchCV(knn, param_grid, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid_search.fit(X_train_scaled_oversampling, y_train_oversampling)  # X_train and y_train are your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = knn_grid_search.best_params_\n",
    "best_model = knn_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_scaled_oversampling)  # X_test is your test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # Define the hyperparameters to optimize\n",
    "    n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 10)\n",
    "    weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
    "\n",
    "    # Create and train the KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
    "    knn.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred = knn.predict(X_test_scaled_oversampling)\n",
    "\n",
    "    # Calculate classification report\n",
    "    report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "    return report\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Create and fit the TPOT classifier\n",
    "ktpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\n",
    "ktpot.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Generate predictions on the test data\n",
    "y_pred = ktpot.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for KNN')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN using bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "# Define the KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define the parameter search space for Bayesian optimization\n",
    "param_space = {\n",
    "    'n_neighbors': (1, 10),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "}\n",
    "\n",
    "# Perform Bayesian optimization with cross-validation\n",
    "opt = BayesSearchCV(knn, param_space, n_iter=20, cv=5)\n",
    "opt.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters: \", opt.best_params_)\n",
    "\n",
    "# Generate predictions on the test data using the best model\n",
    "y_pred = opt.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "data_dmatrix = xgb.DMatrix(data=X_train_scaled_oversampling,label=y_train_oversampling)\n",
    "\n",
    "# import XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# declare parameters\n",
    "params = {\n",
    "            'objective':'binary:logistic',\n",
    "            'max_depth': 4,\n",
    "            'alpha': 10,\n",
    "            'learning_rate': 1.0,\n",
    "            'n_estimators':100\n",
    "        }\n",
    "            \n",
    "            \n",
    "            \n",
    "# instantiate the classifier \n",
    "xgb_clf = XGBClassifier(**params)\n",
    "\n",
    "\n",
    "\n",
    "# fit the classifier to the training data\n",
    "xgb_clf.fit(X_train_scaled_oversampling, y_train_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_clf.predict(X_test_scaled_oversampling)\n",
    "print('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(y_test_oversampling, y_pred)))     \n",
    "#print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import cv\n",
    "\n",
    "params = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "xgb_cv = cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50, early_stopping_rounds=10, metrics=\"auc\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xgb_clf)\n",
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for XGB')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB using randomized search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Example values for n_estimators\n",
    "    'max_depth': [3, 4, 5],           # Example values for max_depth\n",
    "    'learning_rate': [0.1, 0.01, 0.001]  # Example values for learning_rate\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_random_search = RandomizedSearchCV(model, param_grid, n_iter=10, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_random_search.fit(X_train_scaled_oversampling, y_train_oversampling)  # X_train and y_train are your training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = xgb_random_search.best_params_\n",
    "best_model = xgb_random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_scaled_oversampling)  # X_test is your test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Example values for n_estimators\n",
    "    'max_depth': [3, 4, 5],           # Example values for max_depth\n",
    "    'learning_rate': [0.1, 0.01, 0.001]  # Example values for learning_rate\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid_search = GridSearchCV(model, param_grid, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid_search.fit(X_train_scaled_oversampling, y_train_oversampling)  # X_train and y_train are your training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = xgb_grid_search.best_params_\n",
    "best_model = xgb_grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_scaled_oversampling)  # X_test is your test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': 3,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'eta': trial.suggest_loguniform('eta', 1e-3, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train_scaled_oversampling, label=y_train_oversampling)\n",
    "    model = xgb.train(params, dtrain)\n",
    "\n",
    "    dtest = xgb.DMatrix(X_test_scaled_oversampling)\n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    y_pred = y_pred.astype(int)\n",
    "\n",
    "    return classification_report(y_test_oversampling, y_pred, output_dict=True)['weighted avg']['f1-score']\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best trial and its corresponding classification report\n",
    "best_trial = study.best_trial\n",
    "print(\"Best trial:\")\n",
    "print(\"  Value: \", best_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "# Generate predictions on the test data using the best model\n",
    "best_params = best_trial.params\n",
    "model = xgb.train(best_params, xgb.DMatrix(X_train_scaled_oversampling, label=y_train_oversampling))\n",
    "y_pred = model.predict(xgb.DMatrix(X_test_scaled_oversampling))\n",
    "y_pred = y_pred.astype(int)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create and fit the TPOT classifier with XGB estimator\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, template='XGBClassifier')\n",
    "tpot.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Generate predictions on the test data\n",
    "y_pred = tpot.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB using bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the XGB classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Define the parameter search space for Bayesian optimization\n",
    "param_space = {\n",
    "    'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
    "    'max_depth': (3, 10),\n",
    "    'n_estimators': (50, 150),\n",
    "    'gamma': (0.01, 1.0, 'log-uniform'),\n",
    "    'min_child_weight': (1, 10),\n",
    "    'subsample': (0.5, 1.0, 'uniform'),\n",
    "    'colsample_bytree': (0.5, 1.0, 'uniform'),\n",
    "}\n",
    "\n",
    "# Perform Bayesian optimization with cross-validation\n",
    "opt = BayesSearchCV(xgb_classifier, param_space, n_iter=20, cv=5)\n",
    "opt.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters: \", opt.best_params_)\n",
    "\n",
    "# Generate predictions on the test data using the best model\n",
    "y_pred = opt.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the parameter grid with additional parameters\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.5, 0.75, 1],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingClassifier object\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "# Use RandomizedSearchCV to find the best hyperparameters\n",
    "random_gb = RandomizedSearchCV(\n",
    "    estimator=gb,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV object to the data\n",
    "random_gb.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(random_gb.best_params_)\n",
    "\n",
    "# Use the best estimator to make predictions on the test data\n",
    "y_pred = random_gb.best_estimator_.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test set score: {:.4f}'.format(random_gb.score(X_test_scaled_oversampling, y_test_oversampling)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for Gradient boosting classifier')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier using Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the model on the entire training set\n",
    "best_model.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the parameter grid for randomized search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingClassifier object\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Perform randomized search with cross-validation\n",
    "random_search = RandomizedSearchCV(gb_classifier, param_distributions=param_grid, n_iter=10, cv=5, random_state=42)\n",
    "\n",
    "# Fit the training data to the randomized search object\n",
    "random_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Get the best estimator and its parameters\n",
    "best_classifier = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Evaluate the best classifier on the test set\n",
    "accuracy = best_classifier.score(X_test_scaled_oversampling, y_test_oversampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = random_search.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier using  Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.5),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    # Create and fit the GradientBoostingClassifier with the given hyperparameters\n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "    clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "    # Generate predictions on the test data\n",
    "    y_pred = clf.predict(X_test_scaled_oversampling)\n",
    "\n",
    "    # Calculate the F1-score for the weighted average class\n",
    "    report = classification_report(y_test_oversampling, y_pred, output_dict=True)['weighted avg']['f1-score']\n",
    "\n",
    "    return report\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best trial and its corresponding classification report\n",
    "best_trial = study.best_trial\n",
    "print(\"Best trial:\")\n",
    "print(\"  Value: \", best_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "# Train the final GradientBoostingClassifier using the best hyperparameters\n",
    "best_params = best_trial.params\n",
    "clf = GradientBoostingClassifier(**best_params)\n",
    "clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Generate predictions on the test data using the best model\n",
    "y_pred = clf.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create and fit the TPOT classifier with the GradientBoostingClassifier estimator\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, template='GradientBoostingClassifier')\n",
    "tpot.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Generate predictions on the test data using the best model found by TPOT\n",
    "y_pred = tpot.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier using bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_space = {\n",
    "    'n_estimators': (50, 200),\n",
    "    'learning_rate': (0.01, 0.5, 'log-uniform'),\n",
    "    'max_depth': (3, 10),\n",
    "    'min_samples_split': (2, 20),\n",
    "    'min_samples_leaf': (1, 10),\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'max_features': (0.1, 1.0)\n",
    "}\n",
    "\n",
    "# Create and fit the GradientBoostingClassifier with Bayesian optimization\n",
    "opt_model = BayesSearchCV(\n",
    "    GradientBoostingClassifier(),\n",
    "    param_space,\n",
    "    n_iter=50,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    cv=5\n",
    ")\n",
    "opt_model.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Generate predictions on the test data using the best model found by Bayesian optimization\n",
    "y_pred = opt_model.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dt_classifier.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = dt_classifier.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_oversampling, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(dt_classifier.score(X_train_scaled_oversampling, y_train_oversampling)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(dt_classifier.score(X_test_scaled_oversampling, y_test_oversampling)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for decision tree')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree using randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],  # Example values for criterion\n",
    "    'max_depth': [None, 5, 10, 15],  # Example values for max_depth\n",
    "    'min_samples_split': [2, 5, 10],  # Example values for min_samples_split\n",
    "    'min_samples_leaf': [1, 2, 4],  # Example values for min_samples_leaf\n",
    "    'max_features': ['auto', 'sqrt']  # Example values for max_features\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_random_search = RandomizedSearchCV(dt, param_distributions=param_dist, n_iter=10, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_random_search.fit(X_train_scaled_oversampling,y_train_oversampling)  # X_train and y_train are your training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = dt_random_search.best_params_\n",
    "best_model = dt_random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_scaled_oversampling)  # X_test is your test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree using Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Example values for criterion\n",
    "    'max_depth': [None, 5, 10, 15],  # Example values for max_depth\n",
    "    'min_samples_split': [2, 5, 10],  # Example values for min_samples_split\n",
    "    'min_samples_leaf': [1, 2, 4],  # Example values for min_samples_leaf\n",
    "    'max_features': ['auto', 'sqrt']  # Example values for max_features\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid_search = GridSearchCV(dt, param_grid, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid_search.fit(X_train_scaled_oversampling,y_train_oversampling)  # X_train and y_train are your training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = dt_grid_search.best_params_\n",
    "best_model =dt_grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_scaled_oversampling)  # X_test is your test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from optuna import Trial, create_study, study\n",
    "\n",
    "# Create the decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Create the Optuna study\n",
    "study = create_study()\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "  # Set the hyperparameters\n",
    "  max_depth = trial.suggest_int(\"max_depth\", 1, 10)\n",
    "  min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "\n",
    "  # Fit the classifier\n",
    "  clf.set_params(max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "  clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "  # Evaluate the classifier\n",
    "  y_pred = clf.predict(X_test_scaled_oversampling)\n",
    "  accuracy = accuracy_score(y_test_oversampling, y_pred)\n",
    "\n",
    "  # Return the accuracy\n",
    "  return accuracy\n",
    "\n",
    "# Run the optimization\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Fit the classifier with the best parameters\n",
    "clf.set_params(**best_params)\n",
    "clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = clf.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TPOT classifier and specify the generations and population size\n",
    "tpot = TPOTClassifier(generations=10, population_size=50, verbosity=2, \n",
    "                      config_dict={'sklearn.tree.DecisionTreeClassifier': {}})\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Evaluate the classifier on the validation data\n",
    "y_pred = tpot.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree using bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space for hyperparameters\n",
    "param_space = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': (3, 10),\n",
    "    'min_samples_split': (2, 20),\n",
    "    'min_samples_leaf': (1, 10),\n",
    "    'max_features': (0.1, 1.0)\n",
    "}\n",
    "opt_model = BayesSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    param_space,\n",
    "    n_iter=50,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    cv=5\n",
    ")\n",
    "opt_model.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Generate predictions on the test data using the best model found by Bayesian optimization\n",
    "y_pred = opt_model.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Gaussian Naive Bayes classifier on the training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "\n",
    "# fit the model\n",
    "gnb.fit(X_train_scaled_oversampling, y_train_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test_scaled_oversampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test_oversampling, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = gnb.predict(X_train_scaled_oversampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(Gaussian) using randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid for randomized search\n",
    "param_grid = {\n",
    "    'var_smoothing': np.logspace(-10, 0, num=100)\n",
    "}\n",
    "\n",
    "# Create and fit the Gaussian Naive Bayes classifier with randomized search\n",
    "randomized_search = RandomizedSearchCV(\n",
    "    GaussianNB(),\n",
    "    param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='f1_weighted',\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "randomized_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Generate predictions on the test data using the best model found by randomized search\n",
    "y_pred = randomized_search.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(Gaussian) using Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5],\n",
    "    'priors': [None, [0.2, 0.3, 0.5], [0.1, 0.4, 0.5]]\n",
    "    # Add more parameters and their values here\n",
    "}\n",
    "\n",
    "# Create the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(gnb, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = grid_search.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(Gaussian) using Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(Gaussian) using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tpot --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the custom configuration\n",
    "custom_config = {\n",
    "    'sklearn.naive_bayes.GaussianNB': {\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the TPOT classifier with the custom configuration\n",
    "tpot = TPOTClassifier(config_dict=custom_config, generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = tpot.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(Gaussian) using bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Define the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Define the parameter search space\n",
    "param_space = {\n",
    "    'var_smoothing': (1e-9, 1e-3, 'log-uniform')\n",
    "}\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "opt = BayesSearchCV(gnb, param_space, n_iter=20, cv=5, n_jobs=-1)\n",
    "opt.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", opt.best_params_)\n",
    "print(\"Best Score:\", opt.best_score_)\n",
    "\n",
    "# Fit the classifier with the best parameters\n",
    "best_gnb = GaussianNB(var_smoothing=opt.best_params_['var_smoothing'])\n",
    "best_gnb.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = best_gnb.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(BernoulliNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "BernNB = BernoulliNB(binarize=0.0, fit_prior=True, class_prior=None)\n",
    "BernNB.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "print(BernNB)\n",
    "\n",
    "\n",
    "y_pred = BernNB.predict(X_test_scaled_oversampling)\n",
    "\n",
    "\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(BernoulliNB) using randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the BernoulliNB classifier\n",
    "classifier = BernoulliNB()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Create the randomized search object\n",
    "random_search = RandomizedSearchCV(classifier, param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "random_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = random_search.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(BernoulliNB) using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbbernouli = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)\n",
    "}\n",
    "nb_grid_search = GridSearchCV(nbbernouli, parameters)\n",
    "nb_grid_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "nb_grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_oversampling, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(BernoulliNB) using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(BernoulliNB) using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the TPOT classifier\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42, \n",
    "                      config_dict={'sklearn.naive_bayes.BernoulliNB': {}})\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = tpot.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(BernoulliNB) using bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the BernoulliNB classifier\n",
    "classifier = BernoulliNB()\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_space = {\n",
    "    'alpha': (1e-3, 1e-1, 'log-uniform'),\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Create the Bayesian optimization object\n",
    "opt = BayesSearchCV(classifier, param_space, n_iter=50, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "opt.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = opt.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(Multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiNB = MultinomialNB(fit_prior=True, class_prior=None)\n",
    "MultiNB.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "print(MultiNB)\n",
    "\n",
    "\n",
    "y_pred = MultiNB.predict(X_test_scaled_oversampling)\n",
    "\n",
    "\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(Multinomial) using randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the MultinomialNB classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Create the randomized search object\n",
    "random_search = RandomizedSearchCV(classifier, param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "random_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = random_search.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(Multinomial) using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "parameters = {\n",
    "    'alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)\n",
    "}\n",
    "grid_search = GridSearchCV(nb, parameters)\n",
    "grid_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "grid_search.best_params_\n",
    "nbmu_grid_search = MultinomialNB(alpha=1)\n",
    "nbmu_grid_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "pred = nbmu_grid_search.predict(X_test_scaled_oversampling)\n",
    "\n",
    "print(classification_report(y_test_oversampling, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(Multinomial) using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(Multinomial) using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the MultinomialNB classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_space = {\n",
    "    'alpha': (0.001, 1.0, 'log-uniform'),\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Create the BayesSearchCV object\n",
    "bayes_search = BayesSearchCV(classifier, param_space, n_iter=50, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "bayes_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = bayes_search.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES(BernoulliNB) using bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the BernoulliNB classifier\n",
    "classifier = BernoulliNB()\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_space = {\n",
    "    'alpha': (0.001, 1.0, 'log-uniform'),\n",
    "    'binarize': (0.0, 1.0, 'uniform'),\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Create the BayesSearchCV object\n",
    "bayes_search = BayesSearchCV(classifier, param_space, n_iter=50, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "bayes_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = bayes_search.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SVC classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# import metrics to compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# instantiate classifier with default hyperparameters\n",
    "svc=SVC() \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "svc.fit(X_train_scaled_oversampling,y_train_oversampling)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=svc.predict(X_test_scaled_oversampling)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test_oversampling, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate classifier with rbf kernel and C=100\n",
    "svc100=SVC(C=100.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "svc100.fit(X_train_scaled_oversampling,y_train_oversampling)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=svc100.predict(X_test_scaled_oversampling)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with rbf kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test_oversampling, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with rbf kernel and C=1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate classifier with rbf kernel and C=1000\n",
    "svc1000=SVC(C=1000.0) \n",
    "\n",
    "\n",
    "# fit classifier to training set\n",
    "svc1000.fit(X_train_scaled_oversampling,y_train_oversampling)\n",
    "\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred=svc1000.predict(X_test_scaled_oversampling)\n",
    "\n",
    "\n",
    "# compute and print accuracy score\n",
    "print('Model accuracy score with rbf kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(y_test_oversampling, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(svc.score(X_train_scaled_oversampling, y_train_oversampling)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(svc.score(X_test_scaled_oversampling, y_test_oversampling)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM using randomized Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'C': [0.1, 1, 10],  # Example values for C (regularization parameter)\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Example values for kernel type\n",
    "    'gamma': ['scale', 'auto'],  # Example values for gamma (kernel coefficient)\n",
    "    'degree': [2, 3, 4]  # Example values for degree (polynomial degree)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_random_search = RandomizedSearchCV(svm, param_distributions=param_dist, n_iter=10, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_random_search.fit(X_train_scaled_oversampling,y_train_oversampling)  # X_train and y_train are your training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_scaled_oversampling)  # X_test is your test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM using grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = svm.SVC()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "svm_best_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the model on the entire training set\n",
    "svm_best_model.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_best_model.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the TPOT classifier\n",
    "svm_tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "svm_tpot.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = svm_tpot.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_oversampling, y_pred)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['0','1']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for SVM')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add labels to each cell\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM using bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Define the objective function\n",
    "def svm_cv(C, gamma):\n",
    "    # Create the SVM classifier with the specified hyperparameters\n",
    "    svm = SVC(C=C, gamma=gamma, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation with 5 folds\n",
    "    scores = cross_val_score(svm, X_train_scaled_oversampling, y_train_oversampling, cv=5)\n",
    "    \n",
    "    # Return the average accuracy score\n",
    "    return scores.mean()\n",
    "\n",
    "# Define the parameter ranges for tuning\n",
    "param_ranges = {'C': (1e-6, 1e+6), 'gamma': (1e-6, 1e+1)}\n",
    "\n",
    "# Create the BayesianOptimization object\n",
    "svm_bo = BayesianOptimization(svm_cv, param_ranges)\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "svm_bo.maximize(n_iter=10, init_points=5)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = svm_bo.max['params']\n",
    "C = best_params['C']\n",
    "gamma = best_params['gamma']\n",
    "\n",
    "# Create the final SVM classifier with the best hyperparameters\n",
    "svm_final = SVC(C=C, gamma=gamma, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "svm_final.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = svm_final.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron classifier - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of MLPClassifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 100), activation='relu', solver='adam', random_state=42)\n",
    "\n",
    "# Training the classifier on the training set\n",
    "mlp_classifier.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Making predictions on the testing set\n",
    "y_pred = mlp_classifier.predict(X_test_scaled_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP using randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],  # Example values for hidden_layer_sizes\n",
    "    'activation': ['logistic', 'relu'],  # Example values for activation\n",
    "    'solver': ['adam', 'sgd'],  # Example values for solver\n",
    "    'alpha': [0.0001, 0.001, 0.01]  # Example values for alpha\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_random_search.fit(X_train_scaled_oversampling,y_train_oversampling)  # X_train and y_train are your training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = mlp_random_search.best_params_\n",
    "best_model = mlp_random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_scaled_oversampling)  # X_test is your test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['logistic', 'relu'],\n",
    "    'alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Create an MLP classifier\n",
    "mlp_classifier = MLPClassifier()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=mlp_classifier, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "mlp_best_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the model on the entire training set\n",
    "mlp_best_model.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = mlp_best_model.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TPOT classifier\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "y_pred = tpot.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP using bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "pbounds = {\n",
    "    'hidden_layer_sizes': (1, 100),\n",
    "    'alpha': (1e-5, 1e-2),\n",
    "    'learning_rate_init': (1e-4, 1e-2),\n",
    "    'max_iter': (100, 1000)\n",
    "}\n",
    "\n",
    "# Define the objective function for Bayesian optimization\n",
    "def objective(hidden_layer_sizes, alpha, learning_rate_init, max_iter):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=int(hidden_layer_sizes),\n",
    "                        alpha=alpha,\n",
    "                        learning_rate_init=learning_rate_init,\n",
    "                        max_iter=int(max_iter),\n",
    "                        random_state=42)\n",
    "    mlp.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "    y_pred = mlp.predict(X_test_scaled_oversampling)\n",
    "    accuracy = np.mean(y_pred == y_test_oversampling)\n",
    "    return accuracy\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "optimizer = BayesianOptimization(f=objective, pbounds=pbounds, random_state=42)\n",
    "optimizer.maximize(init_points=5, n_iter=10)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = optimizer.max['params']\n",
    "\n",
    "# Fit the MLP classifier with the best hyperparameters\n",
    "mlp = MLPClassifier(hidden_layer_sizes=int(best_params['hidden_layer_sizes']),\n",
    "                    alpha=best_params['alpha'],\n",
    "                    learning_rate_init=best_params['learning_rate_init'],\n",
    "                    max_iter=int(best_params['max_iter']),\n",
    "                    random_state=42)\n",
    "mlp.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = mlp.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Creating an instance of Perceptron\n",
    "p_classifier = Perceptron()\n",
    "\n",
    "# Training the classifier on the training set\n",
    "p_classifier.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Making predictions on the testing set\n",
    "y_pred = p_classifier.predict(X_test_scaled_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron using randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the Perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "    'eta0': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Perform randomized search for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(estimator=perceptron, param_distributions=param_grid, n_iter=10, cv=5)\n",
    "random_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Fit the Perceptron classifier with the best hyperparameters\n",
    "perceptron.set_params(**best_params)\n",
    "perceptron.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = perceptron.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the Perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "    'eta0': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=perceptron, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the Perceptron classifier with the best hyperparameters\n",
    "perceptron.set_params(**best_params)\n",
    "perceptron.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = perceptron.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron using Tpot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the TPOT classifier with the desired configuration\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42, \n",
    "                      config_dict={'sklearn.linear_model.Perceptron': {}},\n",
    "                      scoring='accuracy', cv=5)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = tpot.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron using optuna optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "import optuna\n",
    "\n",
    "# Define the objective function for optimization\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    alpha = trial.suggest_loguniform('alpha', 0.0001, 1.0)\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 1000)\n",
    "    \n",
    "    # Create the Perceptron classifier with the chosen hyperparameters\n",
    "    clf = Perceptron(alpha=alpha, max_iter=max_iter)\n",
    "    \n",
    "    # Fit the classifier to the training data\n",
    "    clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "    \n",
    "    # Predict the labels for the test set\n",
    "    y_pred = clf.predict(X_test_scaled_oversampling)\n",
    "    \n",
    "    # Calculate the accuracy score for evaluation\n",
    "    accuracy = accuracy_score(y_test_oversampling, y_pred)\n",
    "    \n",
    "    # Return the negative accuracy as Optuna aims to minimize the objective function\n",
    "    return -accuracy\n",
    "\n",
    "# Create the Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize the objective function\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Create the Perceptron classifier with the best hyperparameters\n",
    "clf = Perceptron(**best_params)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = clf.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_oversampling, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble(VotingClassifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1 = DecisionTreeClassifier()\n",
    "classifier2 = RandomForestClassifier()\n",
    "classifier3 = LogisticRegression()\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create an ensemble of classifiers using VotingClassifier\n",
    "ensemble_vo = VotingClassifier(estimators=[('dt', classifier1), ('rf', classifier2), ('lr', classifier3)])\n",
    "\n",
    "# Fit the ensemble model on the training data\n",
    "ensemble_vo.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "\n",
    "# Generate predictions on the testing data\n",
    "y_pred = ensemble_vo.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble(BaggingClassifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create the base classifiers\n",
    "classifier1 = DecisionTreeClassifier()\n",
    "classifier2 = RandomForestClassifier()\n",
    "classifier3 = LogisticRegression()\n",
    "\n",
    "# Create the bagging ensemble\n",
    "bagging_classifier = BaggingClassifier(base_estimator=classifier1,\n",
    "                                       n_estimators=10,  # Number of base classifiers\n",
    "                                       random_state=42)\n",
    "\n",
    "# Fit the bagging ensemble to your data\n",
    "bagging_classifier.fit(X_train_scaled_oversampling, y_train_oversampling)  # Replace X and y with your training data\n",
    "\n",
    "# Generate predictions on the testing data\n",
    "y_pred = bagging_classifier.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble(AdaBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create the base classifiers\n",
    "classifier1 = DecisionTreeClassifier()\n",
    "classifier2 = RandomForestClassifier()\n",
    "classifier3 = LogisticRegression()\n",
    "\n",
    "# Create the boosting ensemble\n",
    "boosting_classifier = AdaBoostClassifier(base_estimator=classifier1,\n",
    "                                         n_estimators=10,  # Number of base classifiers\n",
    "                                         random_state=42)\n",
    "\n",
    "# Fit the boosting ensemble to your data\n",
    "boosting_classifier.fit(X_train_scaled_oversampling, y_train_oversampling)  # Replace X and y with your training data\n",
    "\n",
    "# Generate predictions on the testing data\n",
    "y_pred = boosting_classifier.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble(StackingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create the base classifiers\n",
    "classifier1 = DecisionTreeClassifier()\n",
    "classifier2 = RandomForestClassifier()\n",
    "classifier3 = LogisticRegression()\n",
    "\n",
    "# Create the stacking ensemble\n",
    "estimators = [('dt', classifier1), ('rf', classifier2), ('lr', classifier3)]\n",
    "stacking_classifier = StackingClassifier(estimators=estimators,\n",
    "                                         final_estimator=LogisticRegression())\n",
    "\n",
    "# Fit the stacking ensemble to your data\n",
    "stacking_classifier.fit(X_train_scaled_oversampling, y_train_oversampling)  # Replace X and y with your training data\n",
    "\n",
    "# Generate predictions on the testing data\n",
    "y_pred = stacking_classifier.predict(X_test_scaled_oversampling)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_oversampling, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_oversampling, y_pred)\n",
    "\n",
    "# Convert ROC AUC score to percentage\n",
    "roc_auc_percentage = roc_auc * 100\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC score: {roc_auc_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp=plot_roc_curve(rfc_random_search,X_test_scaled_oversampling,y_test_oversampling);\n",
    "plot_roc_curve(tpot ,X_test_scaled_oversampling,y_test_oversampling, ax=disp.ax_);\n",
    "plot_roc_curve(random_gb,X_test_scaled_oversampling,y_test_oversampling, ax=disp.ax_);\n",
    "plot_roc_curve(ktpot,X_test_scaled_oversampling,y_test_oversampling, ax=disp.ax_);\n",
    "plot_roc_curve(svm_tpot,X_test_scaled_oversampling,y_test_oversampling, ax=disp.ax_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=1000),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SVC': SVC(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Define the training set sizes\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)  # Adjust the range according to your needs\n",
    "\n",
    "# Plot the learning curves for each classifier\n",
    "plt.figure(figsize=(10, 8))\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Calculate the learning curve scores\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, X_train_scaled_oversampling, y_train_oversampling, train_sizes=train_sizes, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Calculate the mean of the training and test scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "   \n",
    "\n",
    "    # Plot the learning curve for the current classifier\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=f'{clf_name} (Training Score)')\n",
    "   \n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curves for Multiple Algorithms')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=1000),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SVC': SVC(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Bernoulli Naive Bayes': BernoulliNB(),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'MLP': MLPClassifier(),\n",
    "    'XGB': XGBClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Define the training set sizes\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)  # Adjust the range according to your needs\n",
    "\n",
    "# Plot the learning curves for each classifier\n",
    "plt.figure(figsize=(10, 8))\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Calculate the learning curve scores\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, X_train_scaled_oversampling, y_train_oversampling, train_sizes=train_sizes, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Calculate the mean of the training and test scores\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "   \n",
    "\n",
    "    # Plot the learning curve for the current classifier\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', label=f'{clf_name} (Testing Score)')\n",
    "   \n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Test Set Size')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curves for Multiple Algorithms')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp=plot_roc_curve(svc,X_test_scaled_oversampling,y_test_oversampling)\n",
    "plot_roc_curve(classifier,X_test_scaled_oversampling,y_test_oversampling, ax=disp.ax_);\n",
    "plot_roc_curve(BernNB,X_test_scaled_oversampling,y_test_oversampling, ax=disp.ax_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy before and after feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the accuracies stored in dictionaries\n",
    "classifiers = ['Zero R', 'Random Forest', 'Logistic Regression','KNN', 'XG-Boosting', 'Gradient Boosting','Decision Tree', 'Naive Bayes(Gaussian)', 'Naive Bayes(BernoulliNB)','Naive Bayes(Multinomial)','SVM','MLP','Perceptron','Ensemble (Voting classifier)','Ensemble (Bagging classifier)','Ensemble (ADA boost classifier)','Ensemble (Stacking classifier)']\n",
    "accuracies_before = {'Zero R': 0.49, 'Random Forest': 0.93, 'Logistic Regression': 0.73,'KNN':0.74,'XG-Boosting': 0.89, 'Gradient Boosting':0.89,'Decision Tree':0.84, 'Naive Bayes(Gaussian)':0.69, 'Naive Bayes(BernoulliNB)':0.85,'Naive Bayes(Multinomial)':0.66,'SVM':0.89,'MLP':0.91,'Perceptron':0.70,'Ensemble (Voting classifier)':0.89,'Ensemble (Bagging classifier)':0.85,'Ensemble (ADA boost classifier)':0.81,'Ensemble (Stacking classifier)':0.93}\n",
    "accuracies_after = {'Zero R': 0.49, 'Random Forest': 0.99, 'Logistic Regression': 0.88,'KNN':0.89,'XG-Boosting': 0.95, 'Gradient Boosting':0.93,'Decision Tree':0.95, 'Naive Bayes(Gaussian)':0.80, 'Naive Bayes(BernoulliNB)':0.88,'Naive Bayes(Multinomial)':0.76,'SVM':0.93,'MLP':0.91,'Perceptron':0.80,'Ensemble (Voting classifier)':0.92,'Ensemble (Bagging classifier)':0.92,'Ensemble (ADA boost classifier)':0.86,'Ensemble (Stacking classifier)':0.97}\n",
    "# Create a list of x-axis values (e.g., classifier names)\n",
    "x_values = np.arange(len(classifiers))\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.25\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.bar(x_values, list(accuracies_before.values()), width=bar_width, tick_label=classifiers, label='Before', color='blue')\n",
    "plt.bar(x_values + bar_width, list(accuracies_after.values()), width=bar_width, tick_label=classifiers, label='After', color='orange')\n",
    "\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies of Different Classifiers Before and After Feature Selection')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classifiers = ['Zero R', 'Random Forest', 'Logistic Regression','KNN', 'XG-Boosting', 'Gradient Boosting','Decision Tree', 'Naive Bayes(Gaussian)', 'Naive Bayes(BernoulliNB)','Naive Bayes(Multinomial)','SVM','MLP','Perceptron','Ensemble (Voting classifier)','Ensemble (Bagging classifier)','Ensemble (ADA boost classifier)','Ensemble (Stacking classifier)']\n",
    "accuracies_before = {'Zero R': 0.49, 'Random Forest': 0.97, 'Logistic Regression': 0.94,'KNN':0.92,'XG-Boosting': 0.93, 'Gradient Boosting':0.97,'Decision Tree':0.91, 'Naive Bayes(Gaussian)':0.65, 'Naive Bayes(BernoulliNB)':0.69,'Naive Bayes(Multinomial)':0.72,'SVM':0.97,'MLP':0.97,'Perceptron':0.69,'Ensemble (Voting classifier)':0.97,'Ensemble (Bagging classifier)':0.92,'Ensemble (ADA boost classifier)':0.95,'Ensemble(Stacking)':0.95}\n",
    "\n",
    "x_values = np.arange(len(classifiers))\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.6\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.bar(x_values - bar_width/2, list(accuracies_before.values()), width=bar_width, tick_label=classifiers, label='Oversampling', color='c',edgecolor = \"blue\")\n",
    "\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies of Different Classifiers using oversampling method')\n",
    "plt.xticks(rotation=90)\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lime for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime.lime_tabular\n",
    "from interpret.blackbox import LimeTabular\n",
    "from interpret import show\n",
    "from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = rfc_random_search\n",
    "rf_classifier.fit(X_train_scaled_oversampling, y_train_oversampling)\n",
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train_scaled_oversampling),\n",
    "    feature_names=X_train_scaled_oversampling.columns,\n",
    "    class_names=['Low','High'],\n",
    "    mode='classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_oversampling.iloc[5], y_test_oversampling.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_oversampling.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(\n",
    "    data_row=X_test_scaled_oversampling.iloc[5], \n",
    "    predict_fn=rf_classifier.predict_proba, num_features = 20\n",
    ")\n",
    "\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = exp.as_pyplot_figure()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
